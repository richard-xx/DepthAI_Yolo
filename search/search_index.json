{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-]","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\u57fa\u4e8e\u8bbe\u5907\u4e0a\u89e3\u7801\u7684 Yolo \u68c0\u6d4b","text":"<p>\u8be5\u5b58\u50a8\u5e93 ( \u4fee\u6539\u81ea device-decoding) \u5305\u542b\u76f4\u63a5\u4f7f\u7528 DepthAI SDK (<code>main_sdk.py</code>) \u6216 DepthAI API (<code>main_api.py</code>) \u5728\u8bbe\u5907\u4e0a\u89e3\u7801\u8fd0\u884c Yolo \u76ee\u6807\u68c0\u6d4b\u7684\u4ee3\u7801\u3002\u76ee\u524d\uff0c\u652f\u6301\u7684\u7248\u672c\u6709\uff1a</p> <ul> <li><code>YoloV3</code> &amp; <code>YoloV3-tiny</code>,</li> <li><code>YoloV4</code> &amp; <code>YoloV4-tiny</code>,</li> <li><code>YoloV5</code>,</li> <li><code>YoloV6</code>,</li> <li><code>YoloV7</code>,</li> <li><code>YoloV8</code>,</li> <li><code>YoloV9</code>,</li> <li><code>YoloV10</code> \u3002</li> </ul> <p>\u6211\u4eec\u5728 <code>main_sdk_v*.py(\u4e0d\u63a8\u8350)</code> \u548c <code>main_api.py</code> \u4e2d\u4f7f\u7528\u76f8\u540c\u6837\u5f0f\u7684 JSON \u89e3\u6790\uff0c\u4f46\u60a8\u4e5f\u53ef\u4ee5\u5728\u4ee3\u7801\u4e2d\u624b\u52a8\u8bbe\u7f6e\u8fd9\u4e24\u79cd\u60c5\u51b5\u4e0b\u7684\u503c\u3002</p>"},{"location":"#\u5bfc\u51fa\u6a21\u578b","title":"\u5bfc\u51fa\u6a21\u578b","text":"<p>\u7531\u4e8e\u6a21\u578b\u5fc5\u987b\u4ee5\u67d0\u79cd\u65b9\u5f0f\u5bfc\u51fa\u8f6c\u6362\u5230 OpenVINO IR\uff0c\u6211\u4eec\u63d0\u4f9b\u4e86\u5173\u4e8e\u8bad\u7ec3\u548c\u5bfc\u51fa\u7684\u6559\u7a0b\uff1a</p> <ul> <li> <p><code>YoloV3</code>, <code>YoloV4</code>, \u548c\u5b83\u4eec\u7684 <code>tiny</code> \u7248\u672c\uff1a</p> <ul> <li>\u8bad\u7ec3\uff1a<ul> <li>YoloV3_V4_tiny_training.ipynb </li> <li>https://github.com/AlexeyAB/darknet</li> </ul> </li> <li>\u5bfc\u51fa\u8f6c\u6362\uff1a<ul> <li>https://github.com/luxonis/yolo2openvino</li> </ul> </li> </ul> </li> <li> <p><code>YoloV5</code>, <code>YoloV6</code>, \u548c <code>YoloV7</code> \uff1a</p> <ul> <li>\u8bad\u7ec3\u53ef\u53c2\u8003\u539f\u59cb\u4ed3\u5e93\uff1a<ul> <li>YoloV5,</li> <li>YoloV6,</li> <li>YoloV7</li> <li>YoloV8,</li> <li>YoloV9, YoloV9_ultralytics,</li> <li>YoloV10,</li> <li>YoloV5_training.ipynb </li> <li>YoloV6_training.ipynb </li> <li>YoloV7_training.ipynb </li> <li>YoloV8_training.ipynb </li> </ul> </li> <li>\u5bfc\u51fa\u8f6c\u6362\uff1a<ul> <li>https://tools.luxonis.com/   \u7f51\u9875\u5728\u7ebf\u8f6c\u6362\uff0c</li> <li>\u53c2\u8003 https://github.com/luxonis/tools/tree/master/yolo   \u548c https://github.com/luxonis/tools/tree/master/yolov7   \u8fdb\u884c\u672c\u5730\u8f6c\u6362</li> <li>OAK \u76f8\u673a\u5982\u4f55\u5c06 YOLO \u7cfb\u5217\u6a21\u578b\u8f6c\u6362\u6210 blob \u683c\u5f0f\uff1f</li> </ul> </li> </ul> </li> </ul>"},{"location":"#\u7528\u6cd5","title":"\u7528\u6cd5","text":""},{"location":"#depthai_yolo","title":"depthai_yolo","text":"<p>Usage:</p> <pre><code>depthai_yolo [OPTIONS] [APP]:[oak|lr|sr]\n</code></pre> <p>Options:</p> <pre><code>  [APP]:[oak|lr|sr]               Provide app name for inference  [default:\n                                  oak]\n  -m, -w, --model, --weight PATH  Provide model name or model path for\n                                  inference  [required]\n  -c, -j, --config, --json PATH   Provide config path for inference\n  -classes_id, --classes_id INTEGER\n                                  filter by class: --classes 0 or --classes 0\n                                  2 3\n  -classes_str, --classes_str TEXT\n                                  filter by class: --classes person or\n                                  --classes person cup\n  -usbs, --usbSpeed [UNKNOWN|LOW|FULL|HIGH|SUPER|SUPER_PLUS]\n                                  Force USB communication speed.  [default:\n                                  SUPER_PLUS]\n  --color-res [THE_1080_P|THE_1200_P|THE_4_K|THE_5_MP|THE_12_MP|THE_4000X3000|THE_13_MP|THE_5312X6000|THE_48_MP|THE_720_P|THE_800_P|THE_1440X1080|THE_1352X1012|THE_2024X1520]\n                                  Color camera resolution, if using OAK-LR,\n                                  must be selected from `THE_720_P/THE_400_P`\n                                  (zoom from THE_1200_P).  [default:\n                                  THE_1080_P]\n  --mono-res [THE_720_P|THE_800_P|THE_400_P|THE_480_P|THE_1200_P]\n                                  Mono camera resolution  [default: THE_400_P]\n  -fps, --fps FLOAT               Set capture FPS for all cameras.  [default:\n                                  30]\n  --stereo_pair [LR|LC|CR]        Stereo pair, current only for OAK-LR.\n                                  [default: LR]\n  -s, --spatial                   Display spatial information\n  -lr, --lr_check                 If to True, it will perform left-right check\n                                  on stereo pair, only for `spatial is True`\n                                  [default: True]\n  -e, --extended_disparity        If to True, it will enable disparity, only\n                                  for `spatial is True`\n  -sub, --subpixel                If to True, it will enable subpixel\n                                  disparity, only for `spatial is True`\n  -F, --full_fov / --no_full_fov  If to False, it will first center crop the\n                                  frame to meet the NN aspect ratio and then\n                                  scale down the image  [default: F]\n  -syncNN, --syncNN / --no_syncNN\n                                  Show synced frame  [default: no_syncNN]\n  -high, --high_res / --no_high_res\n                                  Show synced frame  [default: no_high_res]\n  -color, --color / --no_color    Show lens as color, only for `sr`\n                                  [default: no_color]\n  -list, -ls, --list_models       List all pre-defined models\n  -d, --download                  Download all pre-defined models\n  --install-completion            Install completion for the current shell.\n  --show-completion               Show completion for the current shell, to\n                                  copy it or customize the installation.\n</code></pre>"},{"location":"#\u7528\u6cd5-1-\u6a21\u5757\u5b89\u88c5","title":"\u7528\u6cd5 1: \u6a21\u5757\u5b89\u88c5","text":"<ol> <li>\u5b89\u88c5     <pre><code>python3 -m pip install .\n</code></pre></li> <li> <p>\u8fd0\u884c</p> <p>\u53ef\u4ee5\u4f7f\u7528 <code>download_models</code> \u4e0b\u8f7d\u5168\u90e8\u9884\u5b9a\u4e49\u6a21\u578b <code>shell     &gt; python3 -m depthai_yolo.download_models     &gt; # \u6216     &gt; python3 -m depthai_yolo --download     &gt; # \u6216     &gt; depthai_yolo --download     &gt;</code></p> <p>```shell python3 -m depthai_yolo oak -m model_name -c config_json</p> </li> </ol>"},{"location":"#\u6216","title":"\u6216","text":"<p>depthai_yolo api -m model_name -c config_json ```</p> <p>\u82e5\u4f7f\u7528 <code>OAK_D_SR</code> \u8bf7\u8fd0\u884c    <code>shell    python3 -m depthai_yolo sr -m model_name -c config_json    # \u6216    depthai_yolo sr -m model_name -c config_json</code> \u82e5\u4f7f\u7528 <code>OAK_D_LR</code> \u8bf7\u8fd0\u884c    <code>shell    python3 -m depthai_yolo lr -m model_name -c config_json    # \u6216    depthai_yolo lr -m model_name -c config_json</code></p>"},{"location":"#\u7528\u6cd5-2-\u6e90\u7801\u8fd0\u884c","title":"\u7528\u6cd5 2: \u6e90\u7801\u8fd0\u884c","text":"<ol> <li>\u5b89\u88c5\u4f9d\u8d56     <pre><code>python3 -m pip install -r requirements.txt\n</code></pre> <p>\u82e5\u4f7f\u7528 <code>SDK</code> \u8bf7\u8fd0\u884c <code>shell     &gt; python3 -m pip install -r requirements-sdk.txt     &gt;</code></p> </li> <li> <p>\u8fd0\u884c\u811a\u672c     &gt; \u53ef\u4ee5\u4f7f\u7528 <code>download_models.py</code> \u811a\u672c\u4e0b\u8f7d\u9884\u5b9a\u4e49\u6a21\u578b     &gt; <pre><code>python3 -m src/depthai_yolo/download_models.py\n# \u6216\npython3 run.py --download\n</code></pre></p> <pre><code>python3 run.py oak -m model_name -c config_json\n</code></pre> <p>\u82e5\u4f7f\u7528 <code>OAK_D_SR</code> \u8bf7\u8fd0\u884c <code>shell   python3 run.py sr -m model_name -c config_json</code> \u82e5\u4f7f\u7528 <code>OAK_D_LR</code> \u8bf7\u8fd0\u884c <code>`shell   python3 run.py lr -m model_name -c config_json</code></p> </li> </ol>"},{"location":"#\u7528\u6cd5-3-sdk-\u4e0d\u63a8\u8350","title":"\u7528\u6cd5 3: SDK (\u4e0d\u63a8\u8350)","text":"<ol> <li>\u5b89\u88c5\u4f9d\u8d56     <pre><code>python3 -m pip install -r sdk_scripts/requirements-sdk.txt\n</code></pre></li> <li>\u8fd0\u884c\u811a\u672c     <pre><code>python3 sdk_scripts/main_sdk_v1.2.py -m model_name -c config_json\n</code></pre> <pre><code>python3 sdk_scripts/main_sdk_v1.9.py -conf config_json\n</code></pre></li> </ol> <p>\u6ce8\u610f\uff1a</p> <ul> <li><code>model_name</code> \u662f\u6765\u81ea DepthAI \u6a21\u578b\u5e93 (https://zoo.luxonis.com) \u7684\u6a21\u578b\u540d\u79f0\u6216 blob \u6587\u4ef6\u7684\u76f8\u5bf9\u8def\u5f84\u3002     \u8bf7\u67e5\u770b\u6211\u4eec\u7684\u6a21\u578b\u5e93\u4ee5\u67e5\u770b\u53ef\u7528\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u6216\u4f7f\u7528 <code>-ls/--list_models</code> \u53c2\u6570\u67e5\u770b\u53ef\u7528\u6a21\u578b\u3002</li> <li><code>config_json</code> \u662f\u5e26\u6709 Yolo \u6a21\u578b\u5143\u6570\u636e\uff08\u8f93\u5165\u5f62\u72b6\u3001\u951a\u70b9\u3001\u6807\u7b7e\u7b49\uff09\u7684 JSON \u7684\u76f8\u5bf9\u8def\u5f84\u3002</li> </ul>"},{"location":"#jsons","title":"JSONs","text":"<p>\u6211\u4eec\u5df2\u7ecf\u4e3a\u5e38\u89c1\u7684 Yolo \u7248\u672c\u63d0\u4f9b\u4e86\u4e00\u4e9b JSON\u3002\u60a8\u53ef\u4ee5\u7f16\u8f91\u5b83\u4eec\u5e76\u4e3a\u60a8\u7684\u6a21\u578b\u8bbe\u7f6e\u5b83\u4eec\uff0c\u5982\u4e0a\u8ff0\u6559\u7a0b\u4e2d\u7684\u540e\u7eed\u6b65\u9aa4\u90e8\u5206\u6240\u8ff0\u3002\u5982\u679c\u60a8\u8981\u66f4\u6539\u6559\u7a0b\u4e2d\u7684\u67d0\u4e9b\u53c2\u6570\uff0c\u5219\u5e94\u7f16\u8f91\u76f8\u5e94\u7684\u53c2\u6570\u3002\u4e00\u822c\u6765\u8bf4\uff0cJSON \u4e2d\u7684\u8bbe\u7f6e\u5e94\u8be5\u9075\u5faa\u6a21\u578b\u7684 CFG \u4e2d\u7684\u8bbe\u7f6e\u3002\u5bf9\u4e8e YoloV5\uff0c\u9ed8\u8ba4\u8bbe\u7f6e\u5e94\u4e0e YoloV3 \u76f8\u540c\u3002</p> <p>Note\uff1a\u503c\u5fc5\u987b\u4e0e\u8bad\u7ec3\u671f\u95f4\u5728 CFG \u4e2d\u8bbe\u7f6e\u7684\u503c\u76f8\u5339\u914d\u3002\u5982\u679c\u60a8\u4f7f\u7528\u4e0d\u540c\u7684\u8f93\u5165\u5bbd\u5ea6\uff0c\u60a8\u8fd8\u5e94\u8be5\u5c06 <code>side32</code> \u66f4\u6539\u4e3a <code>sideX</code> \u5e76\u5c06 <code>side16</code> \u66f4\u6539\u4e3a <code>sideY</code>\uff0c\u5176\u4e2d <code>X = width16</code> \u548c <code>Y = width32</code>\u3002\u5982\u679c\u60a8\u4f7f\u7528\u7684\u662f\u975e\u5fae\u578b\u6a21\u578b\uff0c\u5219\u8fd9\u4e9b\u503c\u4e3a <code>width8</code>\u3001<code>width16</code> \u548c <code>width32</code>\u3002</p> <p>\u60a8\u8fd8\u53ef\u4ee5\u66f4\u6539 IOU \u548c\u7f6e\u4fe1\u5ea6\u9608\u503c\u3002\u5982\u679c\u591a\u6b21\u68c0\u6d4b\u5230\u540c\u4e00\u4e2a\u76ee\u6807\uff0c\u5219\u589e\u52a0 IOU \u9608\u503c\u3002\u5982\u679c\u6ca1\u6709\u68c0\u6d4b\u5230\u8db3\u591f\u7684\u76ee\u6807\uff0c\u5219\u964d\u4f4e\u7f6e\u4fe1\u5ea6\u9608\u503c\u3002\u8bf7\u6ce8\u610f\uff0c\u8fd9\u4e0d\u4f1a\u795e\u5947\u5730\u6539\u5584\u60a8\u7684\u76ee\u6807\u68c0\u6d4b\u5668\uff0c\u4f46\u5982\u679c\u67d0\u4e9b\u76ee\u6807\u7531\u4e8e\u9608\u503c\u592a\u9ad8\u800c\u88ab\u8fc7\u6ee4\u6389\uff0c\u5219\u53ef\u80fd\u4f1a\u6709\u6240\u5e2e\u52a9\u3002</p>"},{"location":"#depth-\u4fe1\u606f","title":"Depth \u4fe1\u606f","text":"<p>DepthAI \u4f7f\u60a8\u80fd\u591f\u5229\u7528\u6df1\u5ea6\u4fe1\u606f\u5e76\u83b7\u53d6\u68c0\u6d4b\u5230\u7684\u5bf9\u8c61\u7684 <code>x</code>\u3001<code>y</code> \u548c <code>z</code> \u5750\u6807\u3002</p> <pre><code>python3 run.py api -m model_name -c config_json --spatial\n</code></pre> <p>\u6216\u8005</p> <pre><code>python3 main_sdk_v1.2.py -m model_name -c config_json --spatial\n</code></pre> <pre><code>python3 main_sdk_v1.9.py -conf config_json --spatial\n</code></pre> <p>\u5982\u679c\u60a8\u5bf9\u4f7f\u7528 Yolo \u68c0\u6d4b\u5668\u7684\u6df1\u5ea6\u4fe1\u606f\u611f\u5174\u8da3\uff0c \u8bf7\u67e5\u770b\u6211\u4eec\u7684 \u6587\u6863\u3002 </p>"},{"location":"lr/","title":"OAK LR","text":""},{"location":"lr/#\u4ecb\u7ecd","title":"\u4ecb\u7ecd","text":"<p>\u8fd9\u4e2a\u811a\u672c\u7528\u4e8e\u521b\u5efa\u4e00\u4e2a\u57fa\u4e8e depthai \u5e93\u7684 Pipeline\uff0c\u7528\u4e8e\u5904\u7406\u6444\u50cf\u5934\u8f93\u5165\u548c\u76ee\u6807\u68c0\u6d4b\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u3002\u5b83\u53ef\u4ee5\u6839\u636e\u4f20\u5165\u7684\u53c2\u6570\u914d\u7f6e\u6444\u50cf\u5934\u548c\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u5e76\u6784\u5efa\u4e00\u4e2a\u6570\u636e\u6d41\u5904\u7406\u7684 Pipeline\u3002</p>"},{"location":"lr/#\u6e90\u7801","title":"\u6e90\u7801","text":"create_pipeline_for_lr.py <pre><code># coding=utf-8\nfrom enum import Enum\nfrom typing import Union\n\nimport depthai as dai\n\n\nclass StereoPair(str, Enum):\n    \"\"\"\n    Stereo pair for stereo pair inference\n\n    Attributes:\n        LR: CAM_LEFT (CAM_B) as the left camera in the camera pair, and CAM_RIGHT (CAM_C) as the right camera in the camera pair\n        LC: CAM_LEFT (CAM_B) as the left camera in the camera pair, and CAM_CENTER (CAM_A) as the right camera in the camera pair\n        CR: CAM_CENTER(CAM_A) is the left camera in the camera pair, and CAM_RIGHT(CAM_C) is the right camera in the camera pair\n    \"\"\"\n\n    LR = \"LR\"\n    LC = \"LC\"\n    CR = \"CR\"\n\n\ncam_left_socket = {\n    StereoPair.LR: dai.CameraBoardSocket.CAM_B,\n    StereoPair.LC: dai.CameraBoardSocket.CAM_B,\n    StereoPair.CR: dai.CameraBoardSocket.CAM_A,\n}\n\ncam_right_socket = {\n    StereoPair.LR: dai.CameraBoardSocket.CAM_C,\n    StereoPair.LC: dai.CameraBoardSocket.CAM_A,\n    StereoPair.CR: dai.CameraBoardSocket.CAM_C,\n}\n\n\ndef create_pipeline(**kwargs):\n    model_data = kwargs.get(\"model_data\")\n    config_data = kwargs.get(\"config_data\")\n    nn_config = config_data.nn_config\n    color_res = kwargs.get(\"color_res\", dai.ColorCameraProperties.SensorResolution.THE_1200_P)\n    isp_scale = kwargs.get(\"isp_scale\", (2, 3))\n    stereo_pair = kwargs.get(\"stereo_pair\", StereoPair.LR)\n    pipeline = dai.Pipeline()\n\n    cam_rgb = pipeline.create(dai.node.ColorCamera)\n    detection_network = (\n        create_stereo(pipeline, cam_rgb=cam_rgb, **kwargs)\n        if kwargs.get(\"spatial\", False)\n        else pipeline.create(dai.node.YoloDetectionNetwork)\n    )\n\n    xout_image = pipeline.create(dai.node.XLinkOut)\n    nn_out = pipeline.create(dai.node.XLinkOut)\n\n    xout_image.setStreamName(\"image\")\n    nn_out.setStreamName(\"nn\")\n\n    cam_rgb.setPreviewSize(nn_config.nn_width, nn_config.nn_height)\n    cam_rgb.setBoardSocket(cam_left_socket[stereo_pair])\n    cam_rgb.setResolution(color_res)\n    cam_rgb.setIspScale(isp_scale)\n    cam_rgb.setInterleaved(False)\n    cam_rgb.setColorOrder(dai.ColorCameraProperties.ColorOrder.BGR)\n    cam_rgb.setFps(kwargs.get(\"fps\", 30))\n    cam_rgb.setPreviewKeepAspectRatio(not kwargs.get(\"fullFov\", False))\n\n    detection_network.setConfidenceThreshold(nn_config.NN_specific_metadata.confidence_threshold)\n    detection_network.setNumClasses(nn_config.NN_specific_metadata.classes)\n    detection_network.setCoordinateSize(nn_config.NN_specific_metadata.coordinates)\n    detection_network.setAnchors(nn_config.NN_specific_metadata.anchors)\n    detection_network.setAnchorMasks(nn_config.NN_specific_metadata.anchor_masks)\n    detection_network.setIouThreshold(nn_config.NN_specific_metadata.iou_threshold)\n    detection_network.setBlob(model_data)\n    detection_network.input.setBlocking(False)\n    detection_network.input.setQueueSize(1)\n\n    cam_rgb.preview.link(detection_network.input)\n    if kwargs.get(\"syncNN\", False):\n        detection_network.passthrough.link(xout_image.input)\n    elif kwargs.get(\"high_res\", False):\n        cam_rgb.video.link(xout_image.input)\n    else:\n        cam_rgb.preview.link(xout_image.input)\n    detection_network.out.link(nn_out.input)\n\n    return pipeline\n\n\ndef create_stereo(pipeline, **kwargs):\n    color_res = kwargs.get(\"color_res\", dai.ColorCameraProperties.SensorResolution.THE_1200_P)\n    isp_scale = kwargs.get(\"isp_scale\", (2, 3))\n    stereo_pair = kwargs.get(\"stereo_pair\", StereoPair.LR)\n    cam_left = kwargs[\"cam_left\"]\n\n    cam_right = pipeline.create(dai.node.ColorCamera)\n    cam_right.setBoardSocket(cam_right_socket[stereo_pair])\n    cam_right.setResolution(color_res)\n    cam_right.setIspScale(isp_scale)\n    cam_right.setFps(kwargs.get(\"fps\", 30))\n\n    stereo = pipeline.createStereoDepth()\n    stereo.initialConfig.setConfidenceThreshold(245)\n    stereo.setLeftRightCheck(kwargs.get(\"lr_check\", False))\n    stereo.setExtendedDisparity(kwargs.get(\"extended_disparity\", False))\n    stereo.setSubpixel(kwargs.get(\"subpixel\", False))\n    stereo.setDepthAlign(cam_left.getBoardSocket())\n\n    detection_network: Union[dai.node.NeuralNetwork, dai.node.YoloSpatialDetectionNetwork] = pipeline.create(\n        dai.node.YoloSpatialDetectionNetwork\n    )\n    detection_network.setDepthLowerThreshold(100)  # mm\n    detection_network.setDepthUpperThreshold(10_000)  # mm\n    detection_network.setBoundingBoxScaleFactor(0.3)\n\n    cam_left.isp.link(stereo.left)\n    cam_right.isp.link(stereo.right)\n    stereo.depth.link(detection_network.inputDepth)\n\n    return detection_network\n</code></pre>"},{"location":"lr/#\u7528\u6cd5","title":"\u7528\u6cd5","text":"<p>\u8fd9\u4e2a\u811a\u672c\u53ef\u4ee5\u901a\u8fc7\u8c03\u7528<code>create_pipeline</code>\u51fd\u6570\u6765\u521b\u5efa\u4e00\u4e2a Pipeline \u5bf9\u8c61\u3002\u51fd\u6570\u63a5\u53d7\u4ee5\u4e0b\u53c2\u6570\uff1a</p> <ul> <li><code>model_data</code>\uff1a\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u7684\u6570\u636e\u6587\u4ef6\u8def\u5f84\u3002</li> <li><code>config_data</code>\uff1a\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u7684\u914d\u7f6e\u6587\u4ef6\u8def\u5f84\u3002</li> <li><code>spatial</code>\uff1a\u4e00\u4e2a\u5e03\u5c14\u503c\uff0c\u6307\u793a\u662f\u5426\u4f7f\u7528\u7a7a\u95f4\u76ee\u6807\u68c0\u6d4b\u7f51\u7edc\u3002\u9ed8\u8ba4\u4e3a<code>False</code>\u3002</li> <li><code>color_res</code>\uff1a\u5f69\u8272\u6444\u50cf\u5934\u7684\u5206\u8fa8\u7387\u3002\u9ed8\u8ba4\u4e3a<code>dai.ColorCameraProperties.SensorResolution.THE_1200_P</code>\u3002</li> <li><code>isp_scale</code>\uff1a\u56fe\u50cf\u4fe1\u53f7\u5904\u7406\uff08ISP\uff09\u7684\u7f29\u653e\u56e0\u5b50\u3002\u9ed8\u8ba4\u4e3a<code>(2, 3)</code>\u3002</li> <li><code>stereo_pair</code>\uff1a\u7acb\u4f53\u76f8\u673a\u5bf9\u7684\u7c7b\u578b\u3002\u9ed8\u8ba4\u4e3a<code>StereoPair.LR</code>\u3002</li> <li><code>fps</code>\uff1a\u6444\u50cf\u5934\u7684\u5e27\u7387\u3002\u9ed8\u8ba4\u4e3a 30\u3002</li> <li><code>fullFov</code>\uff1a\u4e00\u4e2a\u5e03\u5c14\u503c\uff0c\u6307\u793a\u662f\u5426\u4fdd\u6301\u5168\u89c6\u573a\u6bd4\u4f8b\u3002\u9ed8\u8ba4\u4e3a<code>False</code>\u3002</li> <li><code>syncNN</code>\uff1a\u4e00\u4e2a\u5e03\u5c14\u503c\uff0c\u6307\u793a\u662f\u5426\u540c\u6b65\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u3002\u9ed8\u8ba4\u4e3a<code>False</code>\u3002</li> <li><code>high_res</code>\uff1a\u4e00\u4e2a\u5e03\u5c14\u503c\uff0c\u6307\u793a\u662f\u5426\u4f7f\u7528\u9ad8\u5206\u8fa8\u7387\u9884\u89c8\u3002\u9ed8\u8ba4\u4e3a<code>False</code>\u3002</li> </ul>"},{"location":"lr/#\u793a\u4f8b","title":"\u793a\u4f8b","text":"<p>\u4e0b\u9762\u662f\u4e00\u4e2a\u4f7f\u7528\u793a\u4f8b\uff1a</p> <pre><code>import depthai as dai\nfrom create_pipeline_for_lr import create_pipeline, StereoPair\n\n# \u6307\u5b9a\u6a21\u578b\u6587\u4ef6\u548c\u914d\u7f6e\u6587\u4ef6\u8def\u5f84\nmodel_data = \"path/to/model.blob\"\nconfig_data = \"path/to/config.json\"\n\n# \u521b\u5efa Pipeline\npipeline = create_pipeline(model_data=model_data, config_data=config_data, spatial=True, stereo_pair=StereoPair.LR)\n\n# \u521b\u5efa Device\nwith dai.Device(pipeline) as device:\n    # \u542f\u52a8 Pipeline\n    device.startPipeline()\n\n    # \u83b7\u53d6\u8f93\u51fa\u6d41\n    image_queue = device.getOutputQueue(name=\"image\", maxSize=4, blocking=False)\n    nn_queue = device.getOutputQueue(name=\"nn\", maxSize=4, blocking=False)\n\n    while True:\n        # \u4ece\u8f93\u51fa\u961f\u5217\u4e2d\u83b7\u53d6\u6570\u636e\n        image_data = image_queue.get()\n        nn_data = nn_queue.get()\n\n        # \u5904\u7406\u6570\u636e...\n</code></pre>"},{"location":"lr/#\u5e38\u89c1\u95ee\u9898\u89e3\u7b54","title":"\u5e38\u89c1\u95ee\u9898\u89e3\u7b54","text":"<p>Q: \u8be5\u811a\u672c\u652f\u6301\u54ea\u4e9b\u6df1\u5ea6\u6444\u50cf\u5934\u8bbe\u5907\uff1f</p> <p>A: \u8be5\u811a\u672c\u4f7f\u7528 depthai \u5e93\uff0c\u652f\u6301 OAK-D-LR \u4ee5\u53ca\u4f7f\u7528 <code>AR0234</code> \u76f8\u673a\u6a21\u7ec4\u4f5c\u4e3a\u5de6\u53f3\u76f8\u673a\u7684 OAK-FFC\u3002</p> <p>Q: \u5982\u4f55\u6307\u5b9a\u81ea\u5b9a\u4e49\u7684\u6a21\u578b\u548c\u914d\u7f6e\u6587\u4ef6\uff1f</p> <p>A: \u5728\u8c03\u7528<code>create_pipeline</code>\u51fd\u6570\u65f6\uff0c\u901a\u8fc7<code>model_data</code>\u548c<code>config_data</code>\u53c2\u6570\u6307\u5b9a\u6a21\u578b\u548c\u914d\u7f6e\u6587\u4ef6\u7684\u8def\u5f84\u3002</p> <p>Q: \u662f\u5426\u652f\u6301\u5176\u4ed6\u7c7b\u578b\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff1f</p> <p>A: \u76ee\u524d\u8fd9\u4e2a\u811a\u672c\u652f\u6301 YoloDetectionNetwork \u548c YoloSpatialDetectionNetwork \u4e24\u79cd\u7c7b\u578b\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u3002</p>"},{"location":"oak-d/","title":"OAK D","text":""},{"location":"oak-d/#\u4ecb\u7ecd","title":"\u4ecb\u7ecd","text":"<p>\u8fd9\u4e2a\u811a\u672c\u7528\u4e8e\u521b\u5efa\u4e00\u4e2a\u57fa\u4e8e depthai \u5e93\u7684 Pipeline\uff0c\u7528\u4e8e\u5904\u7406\u6444\u50cf\u5934\u8f93\u5165\u548c\u76ee\u6807\u68c0\u6d4b\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u3002\u5b83\u53ef\u4ee5\u6839\u636e\u4f20\u5165\u7684\u53c2\u6570\u914d\u7f6e\u6444\u50cf\u5934\u548c\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u5e76\u6784\u5efa\u4e00\u4e2a\u6570\u636e\u6d41\u5904\u7406\u7684 Pipeline\u3002</p>"},{"location":"oak-d/#\u6e90\u7801","title":"\u6e90\u7801","text":"create_pipeline_for_oak_d.py <pre><code># coding=utf-8\nfrom typing import Union\n\nimport depthai as dai\n\n\ndef create_pipeline(**kwargs):\n    model_data = kwargs.get(\"model_data\")\n    config_data = kwargs.get(\"config_data\")\n    nn_config = config_data.nn_config\n    pipeline = dai.Pipeline()\n\n    cam_rgb = pipeline.create(dai.node.ColorCamera)\n    detection_network = (\n        create_stereo(pipeline, **kwargs)\n        if kwargs.get(\"spatial\", False)\n        else pipeline.create(dai.node.YoloDetectionNetwork)\n    )\n\n    xout_image = pipeline.create(dai.node.XLinkOut)\n    nn_out = pipeline.create(dai.node.XLinkOut)\n\n    xout_image.setStreamName(\"image\")\n    nn_out.setStreamName(\"nn\")\n\n    cam_rgb.setPreviewSize(nn_config.nn_width, nn_config.nn_height)\n    cam_rgb.setBoardSocket(dai.CameraBoardSocket.CAM_A)\n    cam_rgb.setResolution(kwargs.get(\"color_res\", dai.ColorCameraProperties.SensorResolution.THE_1080_P))\n    cam_rgb.setInterleaved(False)\n    cam_rgb.setColorOrder(dai.ColorCameraProperties.ColorOrder.BGR)\n    cam_rgb.setFps(kwargs.get(\"fps\", 30))\n    cam_rgb.setPreviewKeepAspectRatio(not kwargs.get(\"fullFov\", False))\n\n    detection_network.setConfidenceThreshold(nn_config.NN_specific_metadata.confidence_threshold)\n    detection_network.setNumClasses(nn_config.NN_specific_metadata.classes)\n    detection_network.setCoordinateSize(nn_config.NN_specific_metadata.coordinates)\n    detection_network.setAnchors(nn_config.NN_specific_metadata.anchors)\n    detection_network.setAnchorMasks(nn_config.NN_specific_metadata.anchor_masks)\n    detection_network.setIouThreshold(nn_config.NN_specific_metadata.iou_threshold)\n    detection_network.setBlob(model_data)\n    detection_network.input.setBlocking(False)\n    detection_network.input.setQueueSize(1)\n\n    cam_rgb.preview.link(detection_network.input)\n    if kwargs.get(\"syncNN\", False):\n        detection_network.passthrough.link(xout_image.input)\n    elif kwargs.get(\"high_res\", False):\n        cam_rgb.video.link(xout_image.input)\n    else:\n        cam_rgb.preview.link(xout_image.input)\n    detection_network.out.link(nn_out.input)\n\n    return pipeline\n\n\ndef create_stereo(pipeline, **kwargs):\n    mono_res = kwargs.get(\"mono_res\", dai.MonoCameraProperties.SensorResolution.THE_720_P)\n\n    mono_left = pipeline.createMonoCamera()\n    mono_left.setBoardSocket(dai.CameraBoardSocket.CAM_B)\n    mono_left.setResolution(mono_res)\n    mono_left.setFps(kwargs.get(\"fps\", 30))\n    mono_right = pipeline.createMonoCamera()\n    mono_right.setBoardSocket(dai.CameraBoardSocket.CAM_C)\n    mono_right.setResolution(mono_res)\n    mono_right.setFps(kwargs.get(\"fps\", 30))\n    stereo = pipeline.createStereoDepth()\n    stereo.initialConfig.setConfidenceThreshold(245)\n    stereo.setLeftRightCheck(kwargs.get(\"lr_check\", False))\n    stereo.setExtendedDisparity(kwargs.get(\"extended_disparity\", False))\n    stereo.setSubpixel(kwargs.get(\"subpixel\", False))\n    stereo.setDepthAlign(dai.CameraBoardSocket.CAM_A)\n\n    detection_network: Union[dai.node.NeuralNetwork, dai.node.YoloSpatialDetectionNetwork] = pipeline.create(\n        dai.node.YoloSpatialDetectionNetwork\n    )\n    detection_network.setDepthLowerThreshold(100)  # mm\n    detection_network.setDepthUpperThreshold(10_000)  # mm\n    detection_network.setBoundingBoxScaleFactor(0.3)\n\n    mono_left.out.link(stereo.left)\n    mono_right.out.link(stereo.right)\n    stereo.depth.link(detection_network.inputDepth)\n\n    return detection_network\n</code></pre>"},{"location":"oak-d/#\u7528\u6cd5","title":"\u7528\u6cd5","text":"<p>\u8fd9\u4e2a\u811a\u672c\u53ef\u4ee5\u901a\u8fc7\u8c03\u7528<code>create_pipeline</code>\u51fd\u6570\u6765\u521b\u5efa\u4e00\u4e2a Pipeline \u5bf9\u8c61\u3002\u51fd\u6570\u63a5\u53d7\u4ee5\u4e0b\u53c2\u6570\uff1a</p> <ul> <li><code>model_data</code>\uff1a\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u7684\u6570\u636e\u6587\u4ef6\u8def\u5f84\u3002</li> <li><code>config_data</code>\uff1a\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u7684\u914d\u7f6e\u6587\u4ef6\u8def\u5f84\u3002</li> <li><code>spatial</code>\uff1a\u4e00\u4e2a\u5e03\u5c14\u503c\uff0c\u6307\u793a\u662f\u5426\u4f7f\u7528\u7a7a\u95f4\u76ee\u6807\u68c0\u6d4b\u7f51\u7edc\u3002\u9ed8\u8ba4\u4e3a<code>False</code>\u3002</li> <li><code>color_res</code>\uff1a\u5f69\u8272\u6444\u50cf\u5934\u7684\u5206\u8fa8\u7387\u3002\u9ed8\u8ba4\u4e3a<code>dai.ColorCameraProperties.SensorResolution.THE_1080_P</code>\u3002</li> <li><code>fps</code>\uff1a\u6444\u50cf\u5934\u7684\u5e27\u7387\u3002\u9ed8\u8ba4\u4e3a 30\u3002</li> <li><code>fullFov</code>\uff1a\u4e00\u4e2a\u5e03\u5c14\u503c\uff0c\u6307\u793a\u662f\u5426\u4fdd\u6301\u5168\u89c6\u573a\u6bd4\u4f8b\u3002\u9ed8\u8ba4\u4e3a<code>False</code>\u3002</li> <li><code>syncNN</code>\uff1a\u4e00\u4e2a\u5e03\u5c14\u503c\uff0c\u6307\u793a\u662f\u5426\u540c\u6b65\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u3002\u9ed8\u8ba4\u4e3a<code>False</code>\u3002</li> <li><code>high_res</code>\uff1a\u4e00\u4e2a\u5e03\u5c14\u503c\uff0c\u6307\u793a\u662f\u5426\u4f7f\u7528\u9ad8\u5206\u8fa8\u7387\u9884\u89c8\u3002\u9ed8\u8ba4\u4e3a<code>False</code>\u3002</li> </ul>"},{"location":"oak-d/#\u793a\u4f8b","title":"\u793a\u4f8b","text":"<p>\u4e0b\u9762\u662f\u4e00\u4e2a\u4f7f\u7528\u793a\u4f8b\uff1a</p> <pre><code>import depthai as dai\nfrom create_pipeline_for_oak_d import create_pipeline\n\n# \u6307\u5b9a\u6a21\u578b\u6587\u4ef6\u548c\u914d\u7f6e\u6587\u4ef6\u8def\u5f84\nmodel_data = \"path/to/model.blob\"\nconfig_data = \"path/to/config.json\"\n\n# \u521b\u5efa Pipeline\npipeline = create_pipeline(model_data=model_data, config_data=config_data, spatial=True)\n\n# \u521b\u5efa Device\nwith dai.Device(pipeline) as device:\n    # \u542f\u52a8 Pipeline\n    device.startPipeline()\n\n    # \u83b7\u53d6\u8f93\u51fa\u6d41\n    image_queue = device.getOutputQueue(name=\"image\", maxSize=4, blocking=False)\n    nn_queue = device.getOutputQueue(name=\"nn\", maxSize=4, blocking=False)\n\n    while True:\n        # \u4ece\u8f93\u51fa\u961f\u5217\u4e2d\u83b7\u53d6\u6570\u636e\n        image_data = image_queue.get()\n        nn_data = nn_queue.get()\n\n        # \u5904\u7406\u6570\u636e...\n</code></pre>"},{"location":"oak-d/#\u5e38\u89c1\u95ee\u9898\u89e3\u7b54","title":"\u5e38\u89c1\u95ee\u9898\u89e3\u7b54","text":"<p>Q: \u8be5\u811a\u672c\u652f\u6301\u54ea\u4e9b\u6df1\u5ea6\u6444\u50cf\u5934\u8bbe\u5907\uff1f</p> <p>A: \u8be5\u811a\u672c\u4f7f\u7528 depthai \u5e93\uff0c\u652f\u6301 OAK-D \u7cfb\u5217\u8bbe\u5907\u3002</p> <p>Q: \u5982\u4f55\u6307\u5b9a\u81ea\u5b9a\u4e49\u7684\u6a21\u578b\u548c\u914d\u7f6e\u6587\u4ef6\uff1f</p> <p>A: \u5728\u8c03\u7528<code>create_pipeline</code>\u51fd\u6570\u65f6\uff0c\u901a\u8fc7<code>model_data</code>\u548c<code>config_data</code>\u53c2\u6570\u6307\u5b9a\u6a21\u578b\u548c\u914d\u7f6e\u6587\u4ef6\u7684\u8def\u5f84\u3002</p> <p>Q: \u662f\u5426\u652f\u6301\u5176\u4ed6\u7c7b\u578b\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff1f</p> <p>A: \u76ee\u524d\u8fd9\u4e2a\u811a\u672c\u652f\u6301 YoloDetectionNetwork \u548c YoloSpatialDetectionNetwork \u4e24\u79cd\u7c7b\u578b\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u3002</p>"},{"location":"sr/","title":"OAK SR","text":""},{"location":"sr/#\u4ecb\u7ecd","title":"\u4ecb\u7ecd","text":"<p>\u8fd9\u4e2a\u811a\u672c\u7528\u4e8e\u521b\u5efa\u4e00\u4e2a\u57fa\u4e8e depthai \u5e93\u7684 Pipeline\uff0c\u7528\u4e8e\u5904\u7406\u6444\u50cf\u5934\u8f93\u5165\u548c\u76ee\u6807\u68c0\u6d4b\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u3002\u5b83\u53ef\u4ee5\u6839\u636e\u4f20\u5165\u7684\u53c2\u6570\u914d\u7f6e\u6444\u50cf\u5934\u548c\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u5e76\u6784\u5efa\u4e00\u4e2a\u6570\u636e\u6d41\u5904\u7406\u7684 Pipeline\u3002</p>"},{"location":"sr/#\u6e90\u7801","title":"\u6e90\u7801","text":"create_pipeline_for_sr.py <pre><code># coding=utf-8\nfrom typing import Union\n\nimport depthai as dai\n\n\ndef create_pipeline(**kwargs):\n    model_data = kwargs.get(\"model_data\")\n    config_data = kwargs.get(\"config_data\")\n    nn_config = config_data.nn_config\n    color = kwargs.get(\"color\", False)\n    res = (\n        kwargs.get(\"color_res\", dai.ColorCameraProperties.SensorResolution.THE_720_P)\n        if color\n        else kwargs.get(\"mono_res\", dai.MonoCameraProperties.SensorResolution.THE_720_P)\n    )\n\n    # Create pipeline\n    pipeline = dai.Pipeline()\n\n    # Define sources and outputs\n    mono_right = pipeline.create(dai.node.ColorCamera) if color else pipeline.create(dai.node.MonoCamera)\n    mono_right.setBoardSocket(dai.CameraBoardSocket.CAM_C)\n    mono_right.setResolution(res)\n    mono_right.setFps(kwargs.get(\"fps\", 30))\n\n    detection_network = (\n        create_stereo(pipeline, res=res, mono_right=mono_right, **kwargs)\n        if kwargs.get(\"spatial\", False)\n        else pipeline.create(dai.node.YoloDetectionNetwork)\n    )\n\n    xout_image = pipeline.create(dai.node.XLinkOut)\n    nn_out = pipeline.create(dai.node.XLinkOut)\n\n    xout_image.setStreamName(\"image\")\n    nn_out.setStreamName(\"nn\")\n    if not color:\n        image_manip = pipeline.createImageManip()\n        image_manip.initialConfig.setResize(nn_config.nn_width, nn_config.nn_height)\n        image_manip.initialConfig.setKeepAspectRatio(not kwargs.get(\"fullFov\", False))\n        image_manip.initialConfig.setFrameType(dai.RawImgFrame.Type.BGR888p)\n    else:\n        image_manip = None\n\n    # Network specific settings\n    detection_network.setConfidenceThreshold(nn_config.NN_specific_metadata.confidence_threshold)\n    detection_network.setNumClasses(nn_config.NN_specific_metadata.classes)\n    detection_network.setCoordinateSize(nn_config.NN_specific_metadata.coordinates)\n    detection_network.setAnchors(nn_config.NN_specific_metadata.anchors)\n    detection_network.setAnchorMasks(nn_config.NN_specific_metadata.anchor_masks)\n    detection_network.setIouThreshold(nn_config.NN_specific_metadata.iou_threshold)\n    detection_network.setBlob(model_data)\n    # detection_network.setNumInferenceThreads(2)\n    detection_network.input.setBlocking(False)\n    detection_network.input.setQueueSize(1)\n\n    # Linking\n    if color:\n        mono_right.preview.link(detection_network.input)\n\n    else:\n        mono_right.out.link(image_manip.inputImage)\n        image_manip.out.link(detection_network.input)\n\n    if kwargs.get(\"syncNN\", False):\n        detection_network.passthrough.link(xout_image.input)\n    elif color and kwargs.get(\"high_res\", False):\n        mono_right.video.link(xout_image.input)\n    elif color:\n        mono_right.preview.link(xout_image.input)\n    else:\n        mono_right.out.link(xout_image.input)\n\n    detection_network.out.link(nn_out.input)\n\n    return pipeline\n\n\ndef create_stereo(pipeline, **kwargs):\n    res = kwargs.get(\"res\")\n    mono_right = kwargs.get(\"mono_right\")\n\n    mono_left = (\n        pipeline.create(dai.node.ColorCamera) if kwargs.get(\"color\", False) else pipeline.create(dai.node.MonoCamera)\n    )\n    mono_left.setBoardSocket(dai.CameraBoardSocket.CAM_B)\n    mono_left.setResolution(res)\n    mono_left.setFps(kwargs.get(\"fps\", 30))\n\n    stereo = pipeline.createStereoDepth()\n    stereo.initialConfig.setConfidenceThreshold(245)\n    stereo.setLeftRightCheck(kwargs.get(\"lr_check\", False))\n    stereo.setExtendedDisparity(kwargs.get(\"extended_disparity\", False))\n    stereo.setSubpixel(kwargs.get(\"subpixel\", False))\n    stereo.setDepthAlign(dai.CameraBoardSocket.CAM_C)\n\n    detection_network: Union[dai.node.NeuralNetwork, dai.node.YoloSpatialDetectionNetwork] = pipeline.create(\n        dai.node.YoloSpatialDetectionNetwork\n    )\n    detection_network.setDepthLowerThreshold(100)  # mm\n    detection_network.setDepthUpperThreshold(10_000)  # mm\n    detection_network.setBoundingBoxScaleFactor(0.3)\n\n    mono_left.out.link(stereo.left)\n    mono_right.out.link(stereo.right)\n    stereo.depth.link(detection_network.inputDepth)\n\n    return detection_network\n</code></pre>"},{"location":"sr/#\u7528\u6cd5","title":"\u7528\u6cd5","text":"<p>\u8fd9\u4e2a\u811a\u672c\u53ef\u4ee5\u901a\u8fc7\u8c03\u7528<code>create_pipeline</code>\u51fd\u6570\u6765\u521b\u5efa\u4e00\u4e2a Pipeline \u5bf9\u8c61\u3002\u51fd\u6570\u63a5\u53d7\u4ee5\u4e0b\u53c2\u6570\uff1a</p> <ul> <li><code>model_data</code>\uff1a\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u7684\u6570\u636e\u6587\u4ef6\u8def\u5f84\u3002</li> <li><code>config_data</code>\uff1a\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u7684\u914d\u7f6e\u6587\u4ef6\u8def\u5f84\u3002</li> <li><code>color</code>\uff1a\u4e00\u4e2a\u5e03\u5c14\u503c\uff0c\u6307\u793a\u662f\u5426\u4f7f\u7528\u5f69\u8272\u6444\u50cf\u5934\u3002\u9ed8\u8ba4\u4e3a<code>False</code>\u3002</li> <li><code>color_res</code>\uff1a\u5f69\u8272\u6444\u50cf\u5934\u7684\u5206\u8fa8\u7387\u3002\u9ed8\u8ba4\u4e3a<code>dai.ColorCameraProperties.SensorResolution.THE_720_P</code>\u3002</li> <li><code>mono_res</code>\uff1a\u5355\u8272\u6444\u50cf\u5934\u7684\u5206\u8fa8\u7387\u3002\u9ed8\u8ba4\u4e3a<code>dai.MonoCameraProperties.SensorResolution.THE_720_P</code>\u3002</li> <li><code>fps</code>\uff1a\u6444\u50cf\u5934\u7684\u5e27\u7387\u3002\u9ed8\u8ba4\u4e3a 30\u3002</li> <li><code>spatial</code>\uff1a\u4e00\u4e2a\u5e03\u5c14\u503c\uff0c\u6307\u793a\u662f\u5426\u4f7f\u7528\u7a7a\u95f4\u76ee\u6807\u68c0\u6d4b\u7f51\u7edc\u3002\u9ed8\u8ba4\u4e3a<code>False</code>\u3002</li> <li><code>fullFov</code>\uff1a\u4e00\u4e2a\u5e03\u5c14\u503c\uff0c\u6307\u793a\u662f\u5426\u4fdd\u6301\u5168\u89c6\u573a\u6bd4\u4f8b\u3002\u9ed8\u8ba4\u4e3a<code>False</code>\u3002</li> <li><code>syncNN</code>\uff1a\u4e00\u4e2a\u5e03\u5c14\u503c\uff0c\u6307\u793a\u662f\u5426\u540c\u6b65\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u3002\u9ed8\u8ba4\u4e3a<code>False</code>\u3002</li> <li><code>high_res</code>\uff1a\u4e00\u4e2a\u5e03\u5c14\u503c\uff0c\u6307\u793a\u662f\u5426\u4f7f\u7528\u9ad8\u5206\u8fa8\u7387\u9884\u89c8\u3002\u9ed8\u8ba4\u4e3a<code>False</code>\u3002</li> </ul>"},{"location":"sr/#\u793a\u4f8b","title":"\u793a\u4f8b","text":"<p>\u4e0b\u9762\u662f\u4e00\u4e2a\u4f7f\u7528\u793a\u4f8b\uff1a</p> <pre><code>import depthai as dai\nfrom create_pipeline_for_sr import create_pipeline\n\n# \u6307\u5b9a\u6a21\u578b\u6587\u4ef6\u548c\u914d\u7f6e\u6587\u4ef6\u8def\u5f84\nmodel_data = \"path/to/model.blob\"\nconfig_data = \"path/to/config.json\"\n\n# \u521b\u5efa Pipeline\npipeline = create_pipeline(model_data=model_data, config_data=config_data, color=True)\n\n# \u521b\u5efa Device\nwith dai.Device(pipeline) as device:\n    # \u542f\u52a8 Pipeline\n    device.startPipeline()\n\n    # \u83b7\u53d6\u8f93\u51fa\u6d41\n    image_queue = device.getOutputQueue(name=\"image\", maxSize=4, blocking=False)\n    nn_queue = device.getOutputQueue(name=\"nn\", maxSize=4, blocking=False)\n\n    while True:\n        # \u4ece\u8f93\u51fa\u961f\u5217\u4e2d\u83b7\u53d6\u6570\u636e\n        image_data = image_queue.get()\n        nn_data = nn_queue.get()\n\n        # \u5904\u7406\u6570\u636e...\n</code></pre>"},{"location":"sr/#\u5e38\u89c1\u95ee\u9898\u89e3\u7b54","title":"\u5e38\u89c1\u95ee\u9898\u89e3\u7b54","text":"<p>Q: \u8be5\u811a\u672c\u652f\u6301\u54ea\u4e9b\u6df1\u5ea6\u6444\u50cf\u5934\u8bbe\u5907\uff1f</p> <p>A: \u8be5\u811a\u672c\u4f7f\u7528 depthai \u5e93\uff0c\u652f\u6301 OAK_D\uff0cOAK-D-SR \u4ee5\u53ca\u4f7f\u7528 <code>OV9*82</code> \u76f8\u673a\u6a21\u7ec4\u4f5c\u4e3a\u5de6\u53f3\u76f8\u673a\u7684 OAK-FFC\u3002</p> <p>Q: \u5982\u4f55\u6307\u5b9a\u81ea\u5b9a\u4e49\u7684\u6a21\u578b\u548c\u914d\u7f6e\u6587\u4ef6\uff1f</p> <p>A: \u5728\u8c03\u7528<code>create_pipeline</code>\u51fd\u6570\u65f6\uff0c\u901a\u8fc7<code>model_data</code>\u548c<code>config_data</code>\u53c2\u6570\u6307\u5b9a\u6a21\u578b\u548c\u914d\u7f6e\u6587\u4ef6\u7684\u8def\u5f84\u3002</p> <p>Q: \u662f\u5426\u652f\u6301\u5176\u4ed6\u7c7b\u578b\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff1f</p> <p>A: \u76ee\u524d\u8fd9\u4e2a\u811a\u672c\u652f\u6301 YoloDetectionNetwork \u548c YoloSpatialDetectionNetwork \u4e24\u79cd\u7c7b\u578b\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u3002</p>"}]}